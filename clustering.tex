\section{Parallelism for Regaining Performance}

Deslinkski et al.~\cite{dreslinski2010near} claim ``In applications where there is
an abundance of thread-level parallelism the intention is to use 10 s to 100 s
of NTC processor cores that will regain 10â€“50X of the performance, while
remaining energy efficient.'' In order to regain the performance lost from using
near-threshold techniques, Zhai et al.~\cite{Zhai:2007kn} and Dreslinski et
al.~\cite{Dreslinski:2007id} present a technique for leveraging parallelism.
The proposed architecture groups multiple cores into clusters which share an L1
cache. This is motivated by the observation that SRAM has a higher energy
optimal $V_{dd}$ and $V_{th}$ than logic. Therefore, the energy optimal
frequency of an SRAM is higher than that of logic. Based on this observation,
the proposed technique shares the first-level cache with multiple, slower cores.
The cache operates at $n$ times higher frequency than the cores, where $n$ is
the number of cores in a cluster.  Using this architecture, the cores still
maintain single-cycle memory accesses while the core and memory can operate at
their ideal $V_{dd}$ and $V_{th}$.

Using this technique, a 71\% energy savings over a baseline single core machine
on a highly parallel SPLASH2 benchmark is demonstrated. However, in
investigating these claims, some shortcomings with this approach are revealed.
Of primary concern are the large area overheads required that were required to
achieve the same benchmark performance. In order to achieve the same
performance as the single core system, 6 cores and 3 times the baseline amount
of cache were required. It is important to remember that these results are being
presented in comparison to a single core reference on a highly parallel
workload. By Amdahl's law, the use of parallelism in this case will be most
effective. It would require a larger number of cores to achieve the same level
of parallelism when comparing with a baseline system that included more than a
single core. In fact, to achieve the same performance as the baseline, some
benchmarks require as many as 16 cores and 32 times as much L1 cache (2MB vs
64kB).

The clustering technique also uses separate $V_{th}$ tuning for the core and
cache to find the energy optimal voltage for the same performance. However, in
examining what these voltages actually are, it is revealed both voltages are not
in the near-threshold regime, and are in fact operating at a $V_{dd}$ twice as
high as the selected $V_{tt}$. In modern process technologies, the standard
$V_{dd}$ is approaching 2x $V_{th}$.

One final concern is that no co-optimized configuration is presented for all
benchmarks. This could potentially be an issue as the energy optimal range of
cores, clusters, cache sizes, $V_{dd}$ and $V_{tt}$ is large. It is unclear what
the energy savings across benchmarks for different configurations will be.
